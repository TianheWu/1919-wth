## 周报23

### 本周主要工作

本周的主要工作是写论文，预计在1.15日左右可以有初稿。其次是在代码上花了很多时间去复现。本周复现了IQA与MUSIQ的代码，同时阅读了SWIN-Transformer（保证一周至少要有一篇论文的输入），以及SWIN-Transformer的代码。同时，这周花了一段时间规划了一下之后的科研安排，规划在下周工作中可以体现。

### Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

![image.png](assets/image-20220109221644-hh9iudu.png)

SWIN-T的结构如上图所示：一共包含了4个stage，只有在stage1时会对patches做embedding操作，其余的stage会不断增大维度，缩小高度与宽度，这与CNN的处理机制是一样的，使特征图变得越来越深。(b) 图是Swin Transformer Block的具体结构，主要有LayerNorm，W-MSA，SW-MSA，MLP组成。

#### SwinTransformer 模块

该模块为顶层模块，顶层模块中实现的任务是patch_embeding，加上位置编码，将数据流过4个layer，最后经过Linear层得到分类。

![image.png](assets/image-20220109224409-r1qm3nr.png)

在该模块的代码中，与VIT不同的是，SWIN将位置编码变为了可选项，由`self.ape`控制。

#### PatchEmbedding 模块

![image.png](assets/image-20220109234242-74tnz3b.png)

该模块的作用是将图片切分为patches，并对patches进行嵌入。嵌入的时候，代码中是以patch为单位进行嵌入的，默认维度为96。嵌入后将2，3维度（H，W）进行flatten，并交换了1，2维度->B ph*pw C

#### PatchMerging 模块

该模块的作用是在每个Stage开始前做下采样，缩小分辨率，调整通道数。每一次降采样，使得H和W缩小为原来的1/2。

![image.png](assets/image-20220109235016-yp35r1c.png)

代码318-321行的操作是用stride=2来提取图像信息，然后进行拼接，最后通过全连接层。

#### Window Attention 模块

该模块是SWIN中的核心模块，这里有一个实现难度就是在每一个窗口内加入相对位置编码。由于代码太长就不梳理了。在作者的后续实验中有证明，引入了相对位置编码，模型的性能有所提升。

这一部分的相对位置编码具体算法细节没有搞的很清楚，下周需要继续跟进。

#### Shifted Window Attention 模块

![image.png](assets/image-20220110013025-e9hgkkv.png)

SWIN-T中还加入了Shifted-Window，因为Window-Attention模块仅仅计算了一个Window内的注意力，但是不同Window之间的信息交互，并没有处理，如果将窗口移位，如上图所示，窗口数量会增多。因此，作者设计了特征图移位+mask操作算法，保持了先有的窗口数量，计算不同window之间的注意力。

![image.png](assets/image-20220110013427-tqed6gm.png)

### 下周主要工作

下周的主要工作是：

* **将MUSIQ模型修改为全参考模型，在PIPAL数据集上跑通**
* 在MUSIQ训练部分加入tensorboard可视化操作
* 在MUSIQ模型中加入多GPU分布式训练框架
* 重新理解Transformer代码（具体细节搞明白，现在的问题是理论和代码没有结合起来，还需花时间一点点看）
  * 二次理解SWIN-T，争取对结构以及工作有所思考

下周的主要工作放在代码修改上，同时保证论文的输入。要尽快跑上实验。