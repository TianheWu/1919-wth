## 周报16

### 本周主要工作

本周的第一个主要工作是改良了一下模型，继续跑实验。在跑实验的过程中，我完善了一下半监督中的一致性问题。在输出的结果中设置了一个阈值（为0.2），所有大于0.2的概率值均变为1，小于的话则转变为0。如果直接利用模型预测出的概率结果当作label，那么从宏观角度上看模型其实并没有从与label的loss学习到新东西，因为你的label本身就是模型自己预测出来的。如果将模型预测的结果（soft label）转化为（hard label）的话，相当于这个label是新的，模型根据新的label从loss中学习到了新东西。

第二个工作主要是学习了transformer中的自注意力机制和多头注意力机制。

### 本周实验心得

这周跑的实验效果并不是很显著，尤其是引入半监督和知识蒸馏后，学习效果不增反降，在下周需要着重思考是哪里出现了问题？仔细看看是不是代码写的有问题。同时这周将新网络前面的特征提取部分由resnet18改为resnet50后，发现测试结果还不如resnet18。这说明模型的量级和任务是有一定关系的。任务的计算难度较小，我们也不需要量级大的模型，不能为了追求模型的复杂而错误的将其胡乱应用，要遵循奥卡姆剃刀原则，**如无必要，勿增实体**。

### 模型改良


![image.png](image/image-20211114200419-sikp3qn-2.png)

上周这个网络的参数量为`24x10^6`，而如果将`maxpooling`中的操作减少一回，也就是对应上图中下方的CNN指向MaxPooling的此处减小一次，少做一次池化和卷积，那么参数量则变为`17x10^6`但是效果变差了。如果多加一层`averagePooling`，效果也会变差。

这周的组会中讲到过MAE和iBot，介绍了CV中有没有一种预训练模型像Bert一样，能够成为做任何CV下游任务前的处理方法。因此，我就在想，我设置`averagePooling`的初衷是获得更多的全局感受野，那么应用transformer是否可以？transformer既解决了`Bi-RNN`无法并行计算的问题，又使每一个patch都真正意义上具备了全局感受野，那我是不是可以将transformer引入进来呢？

下周我会继续学习transformer，然后学习一下vision transformer，看看是否有能够借鉴到这里的思路。
