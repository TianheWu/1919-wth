## 周报11 更改网络 + 下阶段思路

### 替换网络

经过两周的努力（第一周仔细捋清楚了数据预处理的流程，这周彻底明确了`train_data`中数据的类型和`output`数据的类型），终于把网络修改成功了，并且在原有的数据集上成功训练上**替换后的网络PSPNet**，成功修改一次后，感觉替换其他的网络也不是很困难了。在我看来，替换网络中最重要的就是：**明确数据的输入与输出**。

更改的地方有如下几点：

* 由于输入的深度图像只具有一个channel，而Resnet的输入是3个channel，因此我在Resnet中加入一个`1x1`的卷积核，将1个channel变为3个channel，是为了符合resnet中的channel数量。选择`1x1`的原因是参数量小，有利于训练过程。
  ```python
  # 该卷积层加在Resnet的init函数中
  self.conv0 = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0, bias=False)
  ```
* 在PSPNet的末尾处加上输出部分，该处的改动是参考ggcnn的输出写法。
  ```python
  # classes是PSPNet中最后部分的输出
  self.pos_output = nn.Conv2d(classes, 1, kernel_size=2)
  self.cos_output = nn.Conv2d(classes, 1, kernel_size=2)
  self.sin_output = nn.Conv2d(classes, 1, kernel_size=2)
  self.width_output = nn.Conv2d(classes, 1, kernel_size=2)
  ```
* 在PSPNet的forward函数中return出上述四个值。以及在forward函数的起始处要对数据的输入的大小进行断言（输入的高和宽均是300）。

#### 训练过程

由于电脑配置问题，为了能够成功训练，将`batech_size`大小改为2（在作者的网络中，最佳的大小应为8）。这里设置为2其实是有问题的，`batch_size`过小会导致训练效果难以收敛。等有服务器后可以设置为8或者16看正常训练效果。

但是也有意外之喜，在`batch_size == 2`的时候，模型最高的准确率高达`88%`，作者的网络准确率最高为`83%`，这说明在前期的想法是正确的，ggcnn的网络结构形似FCN语义分割网络，因此借助PSPNet的金字塔结构得到的效果会更加优秀。

![image.png](assets/image-20211016234707-24qlfs5.png)

我也将这个参数成功保存了下来。我相信，在我选择好合适的`batch_size`后效果会更加优秀，使训练成功收敛。

![image.png](assets/image-20211016234828-bncahp0.png)

### 下一阶段思路与想法

该项目的落脚点在于应用，因此，庞大的模型是不可行的，需要对模型进行量化。因此，在前期我提出了**知识蒸馏**的与**辅助学习**思路。

#### 知识蒸馏

今天的CV组会（讲无监督学习的师姐）提到，训练一个网络最重要的就是找到**如何计算它的loss值**。

这里我的`student model`是`PSPNet with Resnet50`，而`teacher model`是`PSPnet with Resnet101`。在前面的周报中我想到了利用该方法，但是当时没有成功更改网络，也仅仅是思路。这周在成功更改网络后，我仔细地将知识蒸馏的代码思路落实了下来：

* 先训练一个`PSPNet with Resnet101`作为`teacher`
* 在训练`student`的时候输出一个预测值`s_pre`，将其与`label`计算loss得到`s_loss`
* 再利用`teacher`输出一个值`t_val`，用`s_pre`与`t_val`计算一个loss值，得到`st_loss`
* 最终得到的`sum_loss = st_loss + s_loss`进行反向传播。

其实在这里我是了解到这样做可以让轻量级模型学习效果更好，但是其中的数学原理，直到现在依旧在争论。

#### 辅助学习

辅助学习的思路是我在阅读`PSPNet`代码中学习到的，下周我会根据作者添加辅助学习的思路，也为自己的网络中加入辅助学习，提高模型预测的准确率。具体**思路**我在代码中利用注释的形式标明。

```python
    def forward(self, x, y=None):
        ...
	# 在这里作者设置了x_tmp变量（通过金字塔池化模型之前），作为通过layer3的值
        x_tmp = self.layer3(x)
        ...
        if self.training:
	    # 此处作者输出一个辅助学习的值aux
            aux = self.aux(x_tmp)
            if self.zoom_factor != 1:
                aux = F.interpolate(aux, size=(h, w), mode='bilinear', align_corners=True)
	    # 此处，作者计算一个正常预测值与label值的loss，和只通过了resnet三层的model的输出值aux与label的loss
            main_loss = self.criterion(x, y)
            aux_loss = self.criterion(aux, y)
            return x.max(1)[1], main_loss, aux_loss
        else:
            return x
```

在训练过程中，辅助值是这样被用的:

```python
...
output, main_loss, aux_loss = model(input, target)
if not args.multiprocessing_distributed:
    main_loss, aux_loss = torch.mean(main_loss), torch.mean(aux_loss)
# 在计算总loss的值时，作者正常走完一遍网络的loss值和辅助loss值进行了加权求和
loss = main_loss + args.aux_weight * aux_loss
...
```

我个人认为辅助学习能够提高准确率的原因就和**resnet很相似**。由于网络结构较深，因此会存在梯度弥散的情况。在走完一半网络的情况下，直接计算loss与resnet的短接结构相似。最后权衡二者的loss值，训练后的效果会优于未添加辅助学习的模型。

### 算法

这周心思一直投入在更改网络和思考下阶段任务上，因此算法题刷的较少。刷了6道动态规划题。

![image.png](assets/image-20211017010707-1s9qxzu.png)

这周和刘镛学长聊完后，他提到研究生阶段若想尽快提升科研能力和基础水平，就要养成每周读论文的习惯。下周开始我会从最基础的语义分割任务开始每周细读一篇论文，跑通代码，理解内部数学原理。希望我能尽早具备一名准清华研究生该有的科研水平！